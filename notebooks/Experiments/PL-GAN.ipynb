{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unavailable-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "\n",
    "from ilan_src.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP(LightningModule):\n",
    "    def __init__(self,): # fill in\n",
    "        super().__init()\n",
    "        self.lr, self.b1, self.b2 = \n",
    "        self.disc_freq, self.gen_freq = 5, 1\n",
    "        self.noise_shape = \n",
    "        self.lambda_gp = \n",
    "        self.gen = Generator(...)\n",
    "        self.disc = Discriminator(...)\n",
    "        \n",
    "    def forward(self, condition, noise):\n",
    "        return self.gen(condition, noise)\n",
    "    \n",
    "    def gradient_pentalty(self, condition, real, fake):\n",
    "        BATCH_SIZE, C, H, W = real.shape\n",
    "        epsilon = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1,C,H,W).to(device)\n",
    "        interpolated_images = real*epsilon + fake*(1-epsilon)\n",
    "\n",
    "        mixed_scores = self.disc(condition, interpolated_images)\n",
    "        gradient = torch.autograd.grad(\n",
    "                    inputs=interpolated_images,\n",
    "                    outputs=mixed_scores, \n",
    "                    grad_outputs = torch.ones_like(mixed_scores), \n",
    "                    create_graph=True, \n",
    "                    retain_graph = True)[0]\n",
    "\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        gradient_penalty = torch.mean((gradient_norm - 1)**2)\n",
    "        return gradient_penalty\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        real, condition = batch # if label is condition.\n",
    "        \n",
    "        # train discriminator\n",
    "        if optimizer_idx == 0:\n",
    "            noise = torch.randn(real.shape[0], *self.noise_shape)\n",
    "            fake = self.gen(condition, noise)\n",
    "            disc_real = self.disc(condition, real).reshape(-1)\n",
    "            disc_fake = self.disc(condition, fake).reshape(-1)\n",
    "            gp = self.gradient_penalty(condition, real, fake)\n",
    "            loss_disc = -(torch.mean(disc_real) - torch.mean(disc_fake)) + self.lambda_gp*gp\n",
    "            return loss_disc\n",
    "        \n",
    "        #train generator\n",
    "        elif optimizer_idx ==1:\n",
    "            noise = torch.randn(real.shape[0], *self.noise_shape)\n",
    "            fake = self.gen(condition, noise)\n",
    "            gen_fake = self.disc(condition, fake).reshape(-1)\n",
    "            loss_gen = -torch.mean(gen_fake)\n",
    "            return loss_gen        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        gen_opt = optim.Adam(self.disc.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "        disc_opt = optim.Adam(self.disc.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "        return [{\"optimizer\": disc_opt, \"frequency\": self.disc_freq}, {\"optimizer\": gen_opt, \"frequency\": self.gen_freq}]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwp-downscale",
   "language": "python",
   "name": "conda-env-nwp-downscale-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
