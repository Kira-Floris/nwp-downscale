{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "floral-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from ilan_src.models import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "opponent-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGP(LightningModule):\n",
    "    def __init__(self,channels_noise, channels_img, features_g, num_classes, img_size, embed_size, features_d, \n",
    "                      b1 = 0.0, b2 = 0.9, lr = 1e-4, lambda_gp = 10): # fill in\n",
    "        super().__init__()\n",
    "        self.lr, self.b1, self.b2 = lr, b1, b2\n",
    "        self.disc_freq, self.gen_freq = 5, 1\n",
    "        self.noise_shape = (channels_noise, 1, 1)\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.gen = Generator(channels_noise, channels_img, features_g, num_classes, img_size, embed_size)\n",
    "        self.disc = Discriminator(channels_img, features_d, num_classes, img_size)\n",
    "        self.num_to_visualise = 12\n",
    "        self.num_classes = num_classes\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "        \n",
    "    def forward(self, condition, noise):\n",
    "        return self.gen(condition, noise)\n",
    "    \n",
    "    def gradient_penalty(self, condition, real, fake):\n",
    "        BATCH_SIZE, C, H, W = real.shape\n",
    "        epsilon = torch.rand((BATCH_SIZE, 1, 1, 1), device=self.device).repeat(1,C,H,W)\n",
    "        interpolated_images = real*epsilon + fake*(1-epsilon)\n",
    "        interpolated_images.requires_grad = True\n",
    "        mixed_scores = self.disc(condition, interpolated_images)\n",
    "        gradient = torch.autograd.grad(\n",
    "                    inputs=interpolated_images,\n",
    "                    outputs=mixed_scores, \n",
    "                    grad_outputs = torch.ones_like(mixed_scores), \n",
    "                    create_graph=True, \n",
    "                    retain_graph = True)[0]\n",
    "\n",
    "        gradient = gradient.view(gradient.shape[0], -1)\n",
    "        gradient_norm = gradient.norm(2, dim=1)\n",
    "        gradient_penalty = torch.mean((gradient_norm - 1)**2)\n",
    "        return gradient_penalty\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        gen_opt, disc_opt = self.optimizers()\n",
    "        \n",
    "        real, condition = batch # if label is condition.\n",
    "#         print(real.device)\n",
    "        if batch_idx%100==0:\n",
    "            noise = torch.randn(real.shape[0], *self.noise_shape, device=self.device)\n",
    "    #         # log sampled images\n",
    "            sample_imgs = self.gen(condition, noise)\n",
    "            sample_imgs = torch.cat([real, sample_imgs], dim = 0)\n",
    "            grid = torchvision.utils.make_grid(sample_imgs)\n",
    "            self.logger.experiment.add_image('generated_images', grid, batch_idx)\n",
    "        \n",
    "        for _ in range(self.disc_freq):\n",
    "#         # train discriminator\n",
    "#         if optimizer_idx == 0:\n",
    "            noise = torch.randn(real.shape[0], *self.noise_shape, device=self.device)\n",
    "            fake = self.gen(condition, noise)\n",
    "            disc_real = self.disc(condition, real).reshape(-1)\n",
    "            disc_fake = self.disc(condition, fake).reshape(-1)\n",
    "            gp = self.gradient_penalty(condition, real, fake)\n",
    "            loss_disc = -(torch.mean(disc_real) - torch.mean(disc_fake)) + self.lambda_gp*gp\n",
    "#             self.logger.experiment.add_scalar('discriminator_loss', loss_disc, self.current_epoch)\n",
    "            self.log('discriminator_loss', loss_disc, on_epoch=True, on_step=True, prog_bar=True, logger=True)\n",
    "#             return loss_disc\n",
    "            disc_opt.zero_grad()\n",
    "            self.manual_backward(loss_disc, retain_graph=True)\n",
    "            disc_opt.step()\n",
    "        \n",
    "#         #train generator\n",
    "#         elif optimizer_idx ==1:\n",
    "        noise = torch.randn(real.shape[0], *self.noise_shape, device = self.device)\n",
    "        fake = self.gen(condition, noise)\n",
    "        gen_fake = self.disc(condition, fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        self.log('generator_loss', loss_gen, on_epoch=True, on_step=True, prog_bar=True, logger=True)\n",
    "#         return loss_gen \n",
    "        gen_opt.zero_grad()\n",
    "        self.manual_backward(loss_gen)\n",
    "        gen_opt.step()\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        noise = torch.randn(self.num_to_visualise, *self.noise_shape, device = self.device)\n",
    "\n",
    "        # log sampled images\n",
    "        sample_imgs = self(torch.randint(low=0, high = self.num_classes - 1, size=(self.num_to_visualise,), device=self.device), noise)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image('generated_images', grid, self.current_epoch)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        gen_opt = optim.Adam(self.disc.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "        disc_opt = optim.Adam(self.disc.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
    "#         return [{\"optimizer\": disc_opt, \"frequency\": self.disc_freq}, {\"optimizer\": gen_opt, \"frequency\": self.gen_freq}]\n",
    "        return gen_opt, disc_opt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beautiful-investor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "can't set attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-be8123935689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWGANGP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCHANNELS_IMG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEATURES_GEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGEN_EMBEDDING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFEATURES_CRITIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7a16d4f791fe>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, channels_noise, channels_img, features_g, num_classes, img_size, embed_size, features_d, b1, b2, lr, lambda_gp)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_to_visualise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/nwp-downscale/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0mbuffers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m                     \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: can't set attribute"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "IMG_SIZE = 64\n",
    "CHANNELS_IMG = 1\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 30\n",
    "FEATURES_CRITIC = 32 #64\n",
    "FEATURES_GEN = 32 # 64\n",
    "CRITIC_ITERATIONS = 5\n",
    "LAMBDA_GP = 10\n",
    "NUM_CLASSES = 10\n",
    "GEN_EMBEDDING = 100\n",
    "\n",
    "trans = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.MNIST(root=\"dataset/\", transform=trans, download=True)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=6)\n",
    "\n",
    "trainer = pl.Trainer(gpus = 1)\n",
    "model = WGANGP(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMG_SIZE, GEN_EMBEDDING, FEATURES_CRITIC)\n",
    "\n",
    "trainer.fit(model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bacterial-ownership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 124372), started 2:43:02 ago. (Use '!kill 124372' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-64d25184f8b0e247\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-64d25184f8b0e247\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-collins",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nwp-downscale",
   "language": "python",
   "name": "conda-env-nwp-downscale-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
