{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qxb2y1dXWAfw"
   },
   "source": [
    "# SRGAN\n",
    "Notebook to reproduce the results of the SRGAN paper following https://medium.com/analytics-vidhya/super-resolution-gan-srgan-5e10438aec0c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2pxJZvXcoBn",
    "outputId": "081ce914-491b-4dc7-c181-00e96fb672ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Super-Resolution'...\n",
      "remote: Enumerating objects: 10096, done.\u001b[K\n",
      "remote: Total 10096 (delta 0), reused 0 (delta 0), pack-reused 10096\u001b[K\n",
      "Receiving objects: 100% (10096/10096), 65.77 MiB | 26.36 MiB/s, done.\n",
      "Resolving deltas: 100% (3/3), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/vishal1905/Super-Resolution.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "N3asNRupV8NJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as n\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "import os\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RMhygWJlarbK"
   },
   "outputs": [],
   "source": [
    "base_dir = \"Super-Resolution/celeba-dataset/img_align_celeba/img_align_celeba/\"\n",
    "images = os.listdir(base_dir)\n",
    "imageList = images[:1500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giwk1jdSeSa9"
   },
   "source": [
    "## Define generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KYEk24cIahjB"
   },
   "outputs": [],
   "source": [
    "class Generator(n.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = n.Conv2d(3,64,9,padding=4,bias=False)\n",
    "        self.conv2 = n.Conv2d(64,64,3,padding=1,bias=False)\n",
    "        self.conv3_1 = n.Conv2d(64,256,3,padding=1,bias=False)\n",
    "        self.conv3_2 = n.Conv2d(64,256,3,padding=1,bias=False)\n",
    "        self.conv4 = n.Conv2d(64,3,9,padding=4,bias=False)\n",
    "        self.bn = n.BatchNorm2d(64)\n",
    "        self.ps = n.PixelShuffle(2)\n",
    "        self.prelu = n.PReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        block1 = self.prelu(self.conv1(x))\n",
    "        block2 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block1))))),block1)\n",
    "        block3 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block2))))),block2)\n",
    "        block4 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block3))))),block3)\n",
    "        block5 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block4))))),block4)\n",
    "        block6 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block5))))),block5)\n",
    "        block7 = torch.add(self.bn(self.conv2(block6)),block1)\n",
    "        block8 = self.prelu(self.ps(self.conv3_1(block7)))\n",
    "        block9 = self.prelu(self.ps(self.conv3_2(block8)))\n",
    "        block10 = self.conv4(block9)\n",
    "        return block10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zWLHw6G5eV41"
   },
   "outputs": [],
   "source": [
    "gen = Generator().to(cuda).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TqCRB54ej2j"
   },
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "sUZLVPcSebz2"
   },
   "outputs": [],
   "source": [
    "class Discriminator(n.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = n.Conv2d(3,64,3,padding=1,bias=False)\n",
    "        self.conv2 = n.Conv2d(64,64,3,stride=2,padding=1,bias=False)\n",
    "        self.bn2 = n.BatchNorm2d(64)\n",
    "        self.conv3 = n.Conv2d(64,128,3,padding=1,bias=False)\n",
    "        self.bn3 = n.BatchNorm2d(128)\n",
    "        self.conv4 = n.Conv2d(128,128,3,stride=2,padding=1,bias=False)\n",
    "        self.bn4 = n.BatchNorm2d(128)\n",
    "        self.conv5 = n.Conv2d(128,256,3,padding=1,bias=False)\n",
    "        self.bn5 = n.BatchNorm2d(256)\n",
    "        self.conv6 = n.Conv2d(256,256,3,stride=2,padding=1,bias=False)\n",
    "        self.bn6 = n.BatchNorm2d(256)\n",
    "        self.conv7 = n.Conv2d(256,512,3,padding=1,bias=False)\n",
    "        self.bn7 = n.BatchNorm2d(512)\n",
    "        self.conv8 = n.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n",
    "        self.bn8 = n.BatchNorm2d(512)\n",
    "        self.fc1 = n.Linear(512*16*16,1024)\n",
    "        self.fc2 = n.Linear(1024,1)\n",
    "        self.drop = n.Dropout2d(0.3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        block1 = f.leaky_relu(self.conv1(x))\n",
    "        block2 = f.leaky_relu(self.bn2(self.conv2(block1)))\n",
    "        block3 = f.leaky_relu(self.bn3(self.conv3(block2)))\n",
    "        block4 = f.leaky_relu(self.bn4(self.conv4(block3)))\n",
    "        block5 = f.leaky_relu(self.bn5(self.conv5(block4)))\n",
    "        block6 = f.leaky_relu(self.bn6(self.conv6(block5)))\n",
    "        block7 = f.leaky_relu(self.bn7(self.conv7(block6)))\n",
    "        block8 = f.leaky_relu(self.bn8(self.conv8(block7)))\n",
    "        block8 = block8.view(-1,block8.size(1)*block8.size(2)*block8.size(3))\n",
    "        block9 = f.leaky_relu(self.fc1(block8),)\n",
    "        block10 = torch.sigmoid(self.drop(self.fc2(block9)))\n",
    "        return block9,block10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sEwzwNVtenaB"
   },
   "outputs": [],
   "source": [
    "disc = Discriminator().to(cuda).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363009, 138906945)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(gen), count_parameters(disc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z01h6cfNf4VK"
   },
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f18NzuONe0iy"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /home/stephan/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb88ae1fd0b4f5bbb16ccdfd6e3fcb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=574673361.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vgg = models.vgg19(pretrained=True).to(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j_guU5PgFFT"
   },
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JapILj8if7if"
   },
   "outputs": [],
   "source": [
    "gen_loss = n.BCELoss()\n",
    "vgg_loss = n.MSELoss()\n",
    "mse_loss = n.MSELoss()\n",
    "disc_loss = n.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "XYlnRFs9gHZ4"
   },
   "outputs": [],
   "source": [
    "gen_optimizer = optim.Adam(gen.parameters(),lr=0.0001)\n",
    "disc_optimizer = optim.Adam(disc.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnucQrigg_2k"
   },
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nU2bMGmisXBG"
   },
   "source": [
    "### New image loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "NIrmbRayufP3"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False \n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vY0CEV_lufNi"
   },
   "outputs": [],
   "source": [
    "def imagePostProcess(imagedir, modelPath):\n",
    "    \"\"\"\n",
    "    Show model output on unseen images \n",
    "    Parameters:\n",
    "    ----------\n",
    "    imagedir: str\n",
    "        List of paths to unseen images\n",
    "    \n",
    "    \"\"\"\n",
    "    imagelist=[]\n",
    "    original_images = []\n",
    "    for img in imagedir:\n",
    "        img_original = cv2.imread(os.path.join(hr_path,img))\n",
    "        img_original = cv2.resize(img_original, (256,256))\n",
    "        img = degrade_resolution(img_original)\n",
    "        imagelist.append(img)\n",
    "        original_images.append(img_original)\n",
    "    original_images = np.array(original_images)\n",
    "    imagearray = np.array(imagelist)/255\n",
    "    imagearrayPT = np.moveaxis(imagearray,3,1)\n",
    "\n",
    "    model = load_checkpoint(modelPath)\n",
    "    im_tensor = torch.from_numpy(imagearrayPT).float()\n",
    "    out_tensor = model(im_tensor)\n",
    "    out = out_tensor.numpy()\n",
    "    out = np.moveaxis(out,1,3)\n",
    "    out = np.clip(out,0,1)\n",
    "    \n",
    "    return original_images, imagearray, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "W1JqgLL3ufKv"
   },
   "outputs": [],
   "source": [
    "def show_samples(image_dir, model_path):\n",
    "\n",
    "    # Load images and run through\n",
    "    original_images, low_res, out = imagePostProcess(image_dir, model_path)\n",
    "\n",
    "    # Get the number of samples to plot\n",
    "    n_samples = len(image_dir)\n",
    "    figure, axes = plt.subplots(n_samples, 3)\n",
    "    for i in range(n_samples):\n",
    "        axes[i,0].imshow(original_images[i,...][...,::-1])\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        axes[i,1].imshow(low_res[i,...][...,::-1])\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        axes[i,2].imshow(out[i,...][...,::-1])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "YGBT9LeqsY4O"
   },
   "outputs": [],
   "source": [
    "def degrade_resolution(image):\n",
    "    \"\"\"\n",
    "    Degrade image resolution\n",
    "    \"\"\"\n",
    "    resized = cv2.resize(cv2.GaussianBlur(image,(5,5),cv2.BORDER_DEFAULT),(64,64)) \n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "NViG31dVsY1m"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "\n",
    "  def __init__(self, image_list):\n",
    "        'Initialization'\n",
    "        self.image_list = image_list\n",
    "        self.base_dir = \"Super-Resolution/celeba-dataset/img_align_celeba/img_align_celeba/\"\n",
    "\n",
    "  def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # Select sample\n",
    "        image_file = self.image_list[index]\n",
    "\n",
    "        # Load the original (high res) image\n",
    "        high_res = cv2.imread(self.base_dir+image_file)\n",
    "        high_res = cv2.resize(high_res, (256,256))\n",
    "\n",
    "        # Degrade to low res\n",
    "        low_res = degrade_resolution(high_res)\n",
    "\n",
    "        # Normalise\n",
    "        high_res = torch.from_numpy(high_res/255)\n",
    "        low_res = torch.from_numpy(low_res/255)\n",
    "\n",
    "        # Channels to second dim\n",
    "        high_res = high_res.permute(2,0,1)\n",
    "        low_res = low_res.permute(2,0,1)\n",
    "\n",
    "        return low_res.cuda().float(), high_res.cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1gW8WYzosYzI"
   },
   "outputs": [],
   "source": [
    "image_dataset = Dataset(imageList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TV5PxaX2vecT"
   },
   "outputs": [],
   "source": [
    "training_generator = torch.utils.data.DataLoader(image_dataset,\n",
    "                                                 shuffle = True,\n",
    "                                                 batch_size = 32\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tvzNeLGszBbB"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "base_path = os.getcwd()\n",
    "\n",
    "#lr_path = os.path.join(base_path,\"trainImages\")\n",
    "hr_path = base_dir\n",
    "#valid_path = os.path.join(base_path,\"SR_valid\")\n",
    "weight_file = os.path.join(base_path,\"SRPT_weights\")\n",
    "out_path = os.path.join(base_path,\"out\")\n",
    "\n",
    "if not os.path.exists(weight_file):\n",
    "    os.makedirs(weight_file)\n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4YxWY_VFwPYX",
    "outputId": "2698184e-974c-4037-ddc3-143f41aab401"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/47 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 11.17 GiB total capacity; 3.76 GiB already allocated; 234.69 MiB free; 4.15 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-62973fc09819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mgen_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/nwp-downscale/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-aa7676315414>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mblock1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mblock2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mblock3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/nwp-downscale/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_slope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 11.17 GiB total capacity; 3.76 GiB already allocated; 234.69 MiB free; 4.15 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "#batch_count=60\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    d1loss_list=[]\n",
    "    d2loss_list=[]\n",
    "    gloss_list=[]\n",
    "    vloss_list=[]\n",
    "    mloss_list=[]\n",
    "    \n",
    "    for lr_images, hr_images in tqdm(training_generator):\n",
    "                \n",
    "        disc.zero_grad()\n",
    "\n",
    "        gen_out = gen(lr_images)\n",
    "        _,f_label = disc(gen_out)\n",
    "        _,r_label = disc(hr_images)\n",
    "\n",
    "        d1_loss = (disc_loss(f_label,torch.zeros_like(f_label,dtype=torch.float)))\n",
    "        d2_loss = (disc_loss(r_label,torch.ones_like(r_label,dtype=torch.float)))\n",
    "        d2_loss.backward()\n",
    "        d1_loss.backward(retain_graph=True)\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        gen.zero_grad()      \n",
    "        g_loss = gen_loss(f_label.data,torch.ones_like(f_label,dtype=torch.float))\n",
    "        v_loss = vgg_loss(vgg.features[:7](gen_out),vgg.features[:7](hr_images))\n",
    "        m_loss = mse_loss(gen_out,hr_images)\n",
    "        \n",
    "        generator_loss = g_loss + v_loss + m_loss\n",
    "        generator_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        d1loss_list.append(d1_loss.item())\n",
    "        d2loss_list.append(d2_loss.item())\n",
    "        \n",
    "        gloss_list.append(g_loss.item())\n",
    "        vloss_list.append(v_loss.item())\n",
    "        mloss_list.append(m_loss.item())\n",
    "\n",
    "    print(\"d1_loss: \"+str(np.mean(d1loss_list))+\"  d2_loss:\"+str(np.mean(d2loss_list)))\n",
    "    print(\"genLoss: \"+str(np.mean(gloss_list))+\"  vggLoss: \"+str(np.mean(vloss_list))+\"  MeanLoss: \"+str(np.mean(mloss_list)))\n",
    "    \n",
    "    if(epoch%3==0):\n",
    "        \n",
    "        checkpoint = {'model': Generator(),\n",
    "              'input_size': 64,\n",
    "              'output_size': 256,\n",
    "              'state_dict': gen.state_dict()}\n",
    "        torch.save(checkpoint,os.path.join(weight_file,\"SR\"+str(epoch+1)+\".pth\"))\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        show_samples(images[-2:],os.path.join(weight_file,\"SR\"+str(epoch+1)+\".pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiI2rVXuh4cm"
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUHFiTK5lHy_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7j_guU5PgFFT"
   ],
   "machine_shape": "hm",
   "name": "esr_gan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nwp-downscale",
   "language": "python",
   "name": "conda-env-nwp-downscale-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
